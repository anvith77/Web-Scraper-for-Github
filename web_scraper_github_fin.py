# -*- coding: utf-8 -*-
"""web-scraper-github-fin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/166KeSBtOlgkPuTbWKlZuBAtEO6-s5PyV

# Web Scraper Github

Use the "Run" button to execute the code.
"""



"""## Project Outline:
- Website to Scrape - https://github.com/topics
- We will get topic title/page URL/description
- Each topic extracting the top 30 repositories 
- Obtain the Name,Username,Stars and Repo URL for each Repository
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Installing Requests 
- Requests makes handling HTTP requests simple 
- Requests is Open-Source HTTP Library. 
"""

!pip install requests --upgrade 
import requests

"""### Choosing a Website
The Website to be Scraped is: https://github.com/topics
"""

topics_url = "https://github.com/topics"

"""#### HTTP Response Status Code 
- Informational responses (100 – 199)
- Successful responses (200 – 299)
- Redirection messages (300 – 399)
- Client error responses (400 – 499)
- Server error responses (500 – 599)
"""

response =  requests.get(topics_url)
response.status_code

# This line extracts the number of charecters on the selected web page
len(response.text)

"""When we use Python to make a request to a certain URI, a response object is returned. Now, this response object would be used to access certain aspects like content, headers and more. 

 `response.text` returns the content of the response.
"""

page_contents = response.text 
page_contents[:1000]

#response.text
with open('webpage.html', 'w') as f:
    f.write(page_contents)

"""## Installing Beautiful Soup 

Python package called Beautiful Soup is used to parse HTML and XML texts. For parsed pages, it generates a parse tree that can be used to extract HTML data for web scraping.
"""

# installing beautiful soup 
!pip install beautifulsoup4 --upgrade

# From the bs4 module you import the BeautifulSoup Class 
from bs4 import BeautifulSoup

# To parse HTML Code, go to the documentation 
doc = BeautifulSoup(page_contents, 'html.parser')

# It is a BeautifulSoup Object 
type(doc)

"""## Obtaining the Topic Titles, Topic Descriptions and Topic URLs
For the webpage at https://github.com/topics 
"""

# To obtain p tags in general, but it may obtain p_tags (headings/titles) that we don't require
p_tag = doc.find_all('p')
len(p_tag)

# To get the first 3 p-tags 
p_tag[:3]

"""### Extracting the Topic Titles 
`selection_class` contains the a reference to the relevant class in order to obtain the Title.
`topic_title_tags` finds all the paragraphs with respect to the selection_class.
`topic_titles` is a list that contains each topic's title
"""

# <p class="f3 lh-condensed mb-0 mt-1 Link--primary">3D</p>

# In order to get headings in order
# To be more specific with the p_tags that are extracted from the web page 
selection_class = "f3 lh-condensed mb-0 mt-1 Link--primary"
topic_title_tags = doc.find_all('p', {'class': selection_class})
len(topic_title_tags)

topic_titles = [] 
for tag in topic_title_tags:
    topic_titles.append(tag.text)
topic_titles

"""### Extracting the description of each Topic

"""

# <p class="f5 color-fg-muted mb-0 mt-1">

# To get the description in order using the following tag 
selection_tag_desc = "f5 color-fg-muted mb-0 mt-1"
topic_description_tags  = doc.find_all('p' , {'class': selection_tag_desc })
len(topic_description_tags)

topic_description_tags[:2]

topic_descriptions = [] 
for desc in topic_description_tags:
    topic_descriptions.append(desc.text.strip())
topic_descriptions

"""### Obtaining each topic's URL """

#topic_title_tags0.parent
selection_tag_link = "no-underline flex-1 d-flex flex-column"
topic_link_tags = doc.find_all('a', {'class': selection_tag_link})
len(topic_link_tags)

# To get the Topic URL
topic_page_url = "https://github.com" + topic_link_tags[0]['href']
topic_page_url

"""Creating a List called `topic_urls`, which repea"""

topic_urls = [] 
base_url = "https://github.com"
for url in topic_link_tags:
    topic_urls.append(base_url + url['href'])
topic_urls

"""### Importing Pandas to create a Dataframe 

"""

!pip install pandas --upgrade

import pandas as pd

"""#### Combining the Topic Title, Description and URL into a Dictionary"""

topics_dict = {
    'title': topic_titles,
    'description': topic_descriptions,
    'url': topic_urls
}

topics_df  = pd.DataFrame(topics_dict)
topics_df

"""## Getting Information from a Topic Page"""

# Requesting a response from the Topic URL
response = requests.get(topic_page_url)
topic_page_url

response.status_code

# No of characters in the website
len(response.text)

# Using HTML Parser from BeautifulSoup
topic_doc = BeautifulSoup(response.text, 'html.parser')

h3_selection_class = "f3 color-fg-muted text-normal lh-condensed"
repo_tags = topic_doc.find_all('h3', {'class': h3_selection_class})
len(repo_tags)

# Obtaining all the information present in all the a-tags throughout the website
a_tags = repo_tags[0].find_all('a')

# Obtaining the author's name of the Repository - First
a_tags[0].text.strip()

# Obtaining the author's name of the Repository - Second
a_tags[1].text.strip()

# Repository link of the Author 
base_url = "https://github.com"
repo_url = base_url + a_tags[1]['href']
print(repo_url)

# Finding the number of Stars for all Repositories
star_tags = topic_doc.find_all('span', {'class' : "Counter js-social-count"})
len(star_tags)

star_tags[0].text.strip()

# This function converts the 'k' into 1000
# For instance 87k will be converted into 87000

def parse_star_count(stars_str):
    stars_str = stars_str.strip()
    if stars_str[-1] == 'k':
        return int(float(stars_str[:-1])*1000)
    return int(stars_str)

parse_star_count(star_tags[0].text.strip())

def get_repo_info(h1_tag, stars_tag):
    #returns the required information of a repository 
    a_tags = h1_tag.find_all('a')
    username =  a_tags[0].text.strip()
    repo_name = a_tags[1].text.strip()
    repo_url = base_url + a_tags[1]['href']
    stars = parse_star_count(stars_tag.text.strip())
    return username, repo_name, stars, repo_url

get_repo_info(repo_tags[0], star_tags[0])

# Repeating the above steps to get the Author's name, repository name, stars and repository url.
# Feeding this information into a Dictonary 

topic_repos_dict = {
    'username':[],
    'repo_name':[],
    'stars':[],
    'repo_url':[]
}

for i in range(len(repo_tags)):
    repo_info = get_repo_info(repo_tags[i], star_tags[i])
    topic_repos_dict['username'].append(repo_info[0]),
    topic_repos_dict['repo_name'].append(repo_info[1]),
    topic_repos_dict['stars'].append(repo_info[2]),
    topic_repos_dict['repo_url'].append(repo_info[3])

topic_repos_df = pd.DataFrame(topic_repos_dict)
topic_repos_df



"""# Final Code """

def get_topic_page(topic_url):
    #Download the page 
    response = requests.get(topic_url)
    
    # check for succuessful response 
    if response.status_code != 200:
        raise Exception("Failed to load page {}".format(topic_url))
    #Parse Using BeautifulSoup 
    topic_doc = BeautifulSoup(response.text, 'html.parser')
    return topic_doc

def get_repo_info(h1_tag, stars_tag):
    #returns the required information of a repository 
    a_tags = h1_tag.find_all('a')
    username =  a_tags[0].text.strip()
    repo_name = a_tags[1].text.strip()
    repo_url = base_url + a_tags[1]['href']
    stars = parse_star_count(stars_tag.text.strip())
    return username, repo_name, stars, repo_url 

def get_topic_repos(topic_doc):
    
    #Get the h3 Tags containing repo title, repo URL and username
    h3_selection_class = "f3 color-fg-muted text-normal lh-condensed"
    repo_tags = topic_doc.find_all('h3', {'class': h3_selection_class})
    
    #Get Star Tags 
    star_tags = topic_doc.find_all('span', {'class' : "Counter js-social-count"})
    
    topic_repos_dict = {
    'username':[],
    'repo_name':[],
    'stars':[],
    'repo_url':[]
    }

    for i in range(len(repo_tags)):
        repo_info = get_repo_info(repo_tags[i], star_tags[i])
        topic_repos_dict['username'].append(repo_info[0]),
        topic_repos_dict['repo_name'].append(repo_info[1]),
        topic_repos_dict['stars'].append(repo_info[2]),
        topic_repos_dict['repo_url'].append(repo_info[3])
    return pd.DataFrame(topic_repos_dict)

def get_topic_titles(doc):
    #Selection of all Title Tags 
    selection_class = "f3 lh-condensed mb-0 mt-1 Link--primary"
    topic_title_tags = doc.find_all('p', {'class': selection_class})
    
    topic_titles = [] 
    for tag in topic_title_tags:
        topic_titles.append(tag.text)
    return topic_titles

def get_topic_descriptions(doc):
    #Selection of all Description 
    selection_tag_desc = "f5 color-fg-muted mb-0 mt-1"
    topic_description_tags  = doc.find_all('p' , {'class': selection_tag_desc })
    
    topic_descriptions = [] 
    for desc in topic_description_tags:
        topic_descriptions.append(desc.text.strip())
    return topic_descriptions

def get_topic_urls(doc):
    #Selection of all Links 
    selection_tag_link = "no-underline flex-1 d-flex flex-column"
    topic_link_tags = doc.find_all('a', {'class': selection_tag_link})
    
    topic_urls = [] 
    base_url = "https://github.com"
    for url in topic_link_tags:
        topic_urls.append(base_url + url['href'])
    return topic_urls

def scrape_topics():
    topics_url = "https://github.com/topics"
    response =  requests.get(topics_url)
    if response.status_code != 200:
        raise Exception("Failed to load page {}".format(topic_url))

    topics_dict = {
        'title': get_topic_titles(doc),
        'description': get_topic_descriptions(doc),
        'url': get_topic_urls(doc)
    }
    topics_df  = pd.DataFrame(topics_dict)
    return topics_df

topic_repos_df = pd.DataFrame(topic_repos_dict)
topic_repos_df

def get_topic_page(topic_url):
    #Download the page 
    response = requests.get(topic_url)
    
    # check for succuessful response 
    if response.status_code != 200:
        raise Exception("Failed to load page {}".format(topic_url))
    #Parse Using BeautifulSoup 
    topic_doc = BeautifulSoup(response.text, 'html.parser')
    return topic_doc

def get_repo_info(h1_tag, stars_tag):
    #returns the required information of a repository 
    a_tags = h1_tag.find_all('a')
    username =  a_tags[0].text.strip()
    repo_name = a_tags[1].text.strip()
    repo_url = base_url + a_tags[1]['href']
    stars = parse_star_count(stars_tag.text.strip())
    return username, repo_name, stars, repo_url 

def get_topic_repos(topic_doc):
    
    #Get the h3 Tags containing repo title, repo URL and username
    h3_selection_class = "f3 color-fg-muted text-normal lh-condensed"
    repo_tags = topic_doc.find_all('h3', {'class': h3_selection_class})
    
    #Get Star Tags 
    star_tags = topic_doc.find_all('span', {'class' : "Counter js-social-count"})
    
    topic_repos_dict = {
    'username':[],
    'repo_name':[],
    'stars':[],
    'repo_url':[]
    }

    for i in range(len(repo_tags)):
        repo_info = get_repo_info(repo_tags[i], star_tags[i])
        topic_repos_dict['username'].append(repo_info[0]),
        topic_repos_dict['repo_name'].append(repo_info[1]),
        topic_repos_dict['stars'].append(repo_info[2]),
        topic_repos_dict['repo_url'].append(repo_info[3])
    return pd.DataFrame(topic_repos_dict)

def scrape_topic(topic_url, path):
    if os.path.exists(path):
        print("The file {} already exists. Skipping ...".format(path))
        return
    topic_df = get_topic_repos(get_topic_page(topic_url))
    topic_df.to_csv(path + '.csv', index = None)

import os

def scrape_topics_repos():
    topics_df = scrape_topics()
    
    #creating a folder 
    os.makedirs('Data', exist_ok=True)
    
    for index, row in topics_df.iterrows():
        print('Scraping Top Repositories for "{}"'.format(row['title']))
        scrape_topic(row['url'], 'Data/{}'.format(row['title']))

scrape_topics_repos()

